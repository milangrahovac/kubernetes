minikube start/stop
minikube dashboard

kubectl config view

change context to cluster1
kubectl config use-context cluster1

kubectl get pods -n kube-system
NAME                               READY   STATUS    RESTARTS         AGE
coredns-5dd5756b68-9pmh6           1/1     Running   13 (194d ago)    221d
etcd-minikube                      1/1     Running   12 (194d ago)    221d
kube-apiserver-minikube            1/1     Running   12 (14m ago)     221d
kube-controller-manager-minikube   1/1     Running   12 (194d ago)    221d
kube-proxy-rn97l                   1/1     Running   12 (194d ago)    221d
kube-scheduler-minikube            1/1     Running   12 (194d ago)    221d
storage-provisioner                1/1     Running   28 (2m44s ago)   221d

For example ETCDCTL version 2 supports the following commands:
		etcdctl backup
		etcdctl cluster-health
		etcdctl mk
		etcdctl mkdir
		etcdctl set

Whereas the commands are different in version 3
		etcdctl snapshot save
		etcdctl endpoint health
		etcdctl get
		etcdctl put

Certificates:
		--cacert /etc/kubernetes/pki/etcd/ca.crt
		--cert /etc/kubernetes/pki/etcd/server.crt
		--key /etc/kubernetes/pki/etcd/server.key


kubectl run nginx --image nginx
kubectl get pods
kubectl get pods -o wide
kubectl get pods --selector app=app1
kubectl get pod webapp -o yaml > my-new-pod.yaml



kubectl create -f pod-definition.yml
kubectl delete -f pod-definition.yml
kubectl describe pod myapp-pod

kubectl run redis --image=redis --dry-run -0 yaml
kubectl run redis --image=redis --dry-run -0 yaml > redis.yaml

kubectl edit pod <pod-name>
kubectl replace -f replicaset-definition.yml
kubectl replace --force -f replicaset-definition.yml


kubectl get replicationcontroller
kubectl scale replicaset <replicaset-name> --replicas=<new-count>
kubectl slale --replicas=6 -f replicaset-definition.yml
kubectl slale --replicas=6 -f replicaset myapp-replicaset

kubectl create -f replicaset-definition.yml
kubectl get replicaset
kubectl delete  -f replicaset-definition.yml
kubectl delete  replicaset myapp-replicaset

kubectl replace -f replicaset-definition.yml


kubectl logs pod-name

get interactive terminal
kubectl exec-it podname --bin/bash

Create an NGINX Pod
kubectl run nginx --image=nginx

Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)
kubectl run nginx --image=nginx --dry-run=client -o yaml

Create a deployment
kubectl create deployment --image=nginx nginx

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml

Generate Deployment YAML file (-o yaml). Don’t create it(–dry-run) and save it to a file.
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml

Make necessary changes to the file (for example, adding more replicas) and then create the deployment.
kubectl create -f nginx-deployment.yaml

c

OR

In k8s version 1.19+, we can specify the --replicas option to create a deployment with 4 replicas.
kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml


kubectl create deployment --image=httpd:2.4-alpine httpd-frontend --replicas=3 --dry-run=client -o yaml > deployment-definition-2.yaml



kubectl get pods -n kube-system
kubectl get endpoints <service name>   
kubectl get pods --namespace=kube-system
kubectl create -f pode-definition.yml --namespace=dev

kubectl create namespace dev
kubectl config set-context $(kubectl config current-context) --namespace=dev
kubectl get pods --all-namespaces


kubectl run redis --image=redis:alpine --dry-run=client -o yaml > redis-pod.yaml
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3 --dry-run=client -o yam

kubectl run custom-nginx --image=nginx --dry-run=client
kubectl expose pod custom-nginx --port=8080 --name nginx-service --dry-run=client -o yaml
kubectl run custom-nginx --image=nginx --port=8080
kubectl create namespace dev-ns


kubectl create deployment redis-deploy --image=redis --replicas=2 -n=dev-ns --dry-run=client -o yaml


kubectl run httpd --image=httpd:alpine --port=8080
kubectl expose pod httpd --port=80 --name httpd --dry-run=client -o yaml

Taint:
kubect taint nodes node-name key=value:taint-effect
kubectl taint nodes node1 app=blue:NoSchedule

Telerations:
	spec:
		tolerations:
			- key: app
			operator: "Equal"
			value: blue
			effect: NoSchedule


kubectl describe node kubemaster | grep Taint


kubectl label nodes node-name label-key=label-value
kubectl label nodes node01 size=Large 
kubectl label nodes node01 color=blue

kubectl get daemonsets --all-namespaces
kubectl describe daemonset kube-proxy -n kube-system

1. kubectl run nginx-pod --image=nginx:alpine  

2. kubectl run redis --image=redis:alpine --label="tier=db, env=prod"

3. kubectl expose pod redis --port=6379 --name redis-service  

4. kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3

5. kubectl run custom-nginx --image=nginx --port=8080

6. kubectl create namespace dev-ns

7. kubectl create deployment redis-deploy --image=redis --replicas=2 -n=dev-ns

8. kubectl run httpd --image=httpd:alpine --port=80 --expose=true


labels and selectors

1. kubectl get pods --selector env=dev
2. kubectl get pods --selector bu=finance
3. kubectl get all --selector env=prod --no-headers | wc -l
4. kubectl get pods --selector "env=prod,bu=finance,tier=frontend"
5. kubectl apply -f replicaset-definition-1.yaml


1. kubectl get pods --selector env=dev | wc -l
1. kubectl get pods --selector env=dev --no-headers | wc -l


taints
2. kubectl describe node node01 | grep Taint
3. kubectl taint nodes node01 spray=mortein:NoSchedule
4. kubectl run mosquito --image=nginx
7. kubectl run bee --image=nginx
8. kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-


kubectl create deployment red --image=nginx --replicas=3 --dry-run=client -o yaml > blue.yaml

kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3 --dry-run=client -o yam


How is ETCD configured for cluster2?

Remember, you can access the clusters from student-node using the kubectl tool. 
You can also ssh to the cluster nodes from the student-node.
Make sure to switch the context to cluster2:

ssh cluster2-controlplane 
ls /etc/kubernetes/manifests/
kubectl describe pod kube-apiserver-cluster2-controlplane -n kube-system 
check       --etcd-servers=https://192.18.149.12:2379
this is external server


kubectl config use-context cluster2
ps -ef | grep -i etcd
 /usr/local/bin/etcd --name etcd-server --data-dir=/var/lib/etcd-data 
 --cert-file=/etc/etcd/pki/etcd.pem 
 --key-file=/etc/etcd/pki/etcd-key.pem 
 --peer-cert-file=/etc/etcd/pki/etcd.pem 
 --peer-key-file=/etc/etcd/pki/etcd-key.pem 
 --trusted-ca-file=/etc/etcd/pki/ca.pem 
 --peer-trusted-ca-file=/etc/etcd/pki/ca.pem 
 --peer-client-cert-auth 
 --client-cert-auth
--initial-advertise-peer-urls https://192.18.149.12:2380 
--listen-peer-urls https://192.18.149.12:2380 
--advertise-client-urls https://192.18.149.12:2379 
--listen-client-urls https://192.18.149.12:2379,https://127.0.0.1:2379 
--initial-cluster-token etcd-cluster-1 
--initial-cluster etcd-server=https://192.18.149.12:2380 
--initial-cluster-state new


How many nodes are part of the ETCD cluster that etcd-server is a part of?
ETCDCTL_API=3 etcdctl --endpoints=127.0.0.1:2379 --cacert=/etc/etcd/pki/ca.pem --cert=/etc/etcd/pki/etcd.pem --key=/etc/etcd/pki/etcd-key.pem member list

return:
a5ea15b1d2ab8e57, started, etcd-server, https://192.18.149.12:2380, https://192.18.149.12:2379, false



Take a backup of etcd on cluster1 and save it on the student-node at the path /opt/cluster1.db



If needed, make sure to set the context to cluster1 (on the student-node):
ETCDCTL_API=3 etcdctl snapshot save /opt/cluster1.db --endpoints=192.18.149.20:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key

 scp cluster1-controlplane:/opt/cluster1.db /opt/


 An ETCD backup for cluster2 is stored at /opt/cluster2.db. Use this snapshot file to carryout a restore on cluster2 to a new path /var/lib/etcd-data-new.



Once the restore is complete, ensure that the controlplane components on cluster2 are running.


The snapshot was taken when there were objects created in the critical namespace on cluster2. These objects should be available post restore.


If needed, make sure to set the context to cluster2 (on the student-node):

student-node ~ ➜  kubectl config use-context cluster2
Switched to context "cluster2".

scp /opt/cluster2.db etcd-server:/root

ETCDCTL_API=3 etcdctl snapshot restore /root/cluster2.db --data-dir=/var/lib/etcd-data-new
chown -R etcd:etcd etcd-data-new/

change dir path
nano /etc/systemd/system/etcd.service 
systemctl daemon-reload
systemctl restart etcd

kubectl delete pod kube-apiserver-cluster2-controlplane -n kube-system
kubectl delete pod kube-controller-manager-cluster2-controlplane -n kube-system 
kubectl delete pod kube-scheduler-cluster2-controlplane -n kube-system 
ssh cluster2-controlplane
systemctl restart kubelet
=======

kubectl create deamonset elasticsearch --image=registry.k8s.io/fluentd-elasticsearch:1.2 -n=kube-system --dry-run=client -o yaml > blue.yaml
kubectl run static-busybox --image=busybox --dry-run -0 yaml > static-busybox.yaml
kubectl run static-busybox --image=busybox --dry-run=client -o yaml
kubectl set image deployments/frontend simple-webapp=kodekloud/webapp-color:v2
kubectl create deployment --image=kodekloud/webapp-color:v2 frontend --dry-run=client -o yaml > nginx-deployment.yaml
kubectl create configmap 
kubectl create configmap app-config --from-literal=APP_COLOR=blue
kubectl create configmap webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard --dry-run=client -o yaml > webapp-config-map.yaml
kubectl create secrets generic app-secret --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard --dry-run=client -o yaml > webapp-config-map.yaml

kubectl create secrets generic app-secret --from-file=app_secret.properties
echo -n 'mysql' | base64
echo -n 'bXlzcWw=' | base64 --decode

kubectl get secret app-secret -o yaml

kubectl run webapp --image=busybox -- --color green

Summarize Commands
> kubectl rollout status deployment/myapp-deployment
> kubectl rollout history deployment/myapp-deployment
> kubectl create –f deployment-definition.yml
> kubectl get deployments
> kubectl apply –f deployment-definition.yml
> kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1
> kubectl rollout undo deployment/myapp-deployment

kubectl create secrets generic db-secret --from-literal=DB_Host=sql01 \
										 --from-literal=DB_User=root \
										 --from-literal=DB_Password=password123 \
										 --dry-run=client -o yaml > webapp-scr.yaml


kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123 --dry-run=client -o yaml
kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret


critcl pods

kubectl get networkpolicies.networking.k8s.io


how to access pod
kubectl exec webapp -- cat /log/app.log

kubectl get pv
kubectl get pvc
