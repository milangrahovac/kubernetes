kubectl get --all-namespaces -o yaml > all-deploy-services.yaml

create and check snapshot
ETCD_API=3 etcdctl snapshot save snapshot.db
ETCD_API=3 etcdctl snapshot status snapshot.db

restore snapshot
1. stop service
service kube-apiserver stop
2. restore backup
ETCD_API=3 etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup
3. reload deamon
systemctl deamon-reload
service etcd restart
service kube-apiserver start


In all our Kubernetes Hands-on labs, the ETCD key-value database is deployed as a static pod 
on the master. The version used is v3. To make use of etcdctl for tasks such as back up and restore, 
make sure that you set the ETCDCTL_API to 3.
You can do this by exporting the variable ETCDCTL_API prior to using the etcdctl client. 
This can be done as follows:

export ETCDCTL_API=3

To see all the options for a specific sub-command, make use of the -h or --help flag.

For example, if you want to take a snapshot of etcd, use:

etcdctl snapshot save -h and keep a note of the mandatory global options.

Since our ETCD database is TLS-Enabled, the following options are mandatory:

--cacert     verify certificates of TLS-enabled secure servers using this CA bundle

--cert       identify secure client using this TLS certificate file

--endpoints=[127.0.0.1:2379]     This is the default as ETCD is running on master node and exposed on localhost 2379.

--key        identify secure client using this TLS key file



Similarly use the help option for snapshot restore to see all available options for restoring the backup.
etcdctl snapshot restore -h

For a detailed explanation on how to make use of the etcdctl command line tool and work with the -h flags, 
check out the solution video for the Backup and Restore Lab.






134. lab - kubernetes backup and restore 

2. kubectl describe pod etcd-controlplane -n kube-system
6. The master node in our cluster is planned for a regular maintenance reboot tonight. 
While we do not anticipate anything to go wrong, we are required to take the necessary backups. 
Take a snapshot of the ETCD database using the built-in snapshot functionality.
Store the backup file at location /opt/snapshot-pre-boot.db


ls var/lib/etcd 
cat var/lib/etcd.etcd.yaml
create a snapshot

export ETCDCTL_API=3

etcdctl snapshot save \
--endpoints=127.0.0.1:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
/opt/snapshot-pre-boot.db

9. Luckily we took a backup. Restore the original state of the cluster using the backup file.

restore snapshot
etcdctl snapshot restore /opt/snapshot-pre-boot.db --data-dir /var/lib/etcd-from-backup

nano /etc/kubernetes/manifests/etcd.yaml 
change etcd-data path to /var/lib/etcd-from-backup

may requre restart
kubectl delete pod etcd-controlplane -n kube-system


3. reload deamon
systemctl deamon-reload
service etcd restart
service kube-apiserver start