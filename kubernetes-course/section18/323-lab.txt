1. Create a new service account with the name pvviewer. Grant this Service account access 
to list all PersistentVolumes in the cluster by creating an appropriate cluster role 
called pvviewer-role and ClusterRoleBinding called pvviewer-role-binding.
Next, create a pod called pvviewer with the image: redis and serviceAccount: 
pvviewer in the default namespace.

kubectl create serviceaccount pvviewer
kubectl create clusterrole pvviewer-role --verb=list -resource=persistentvolumes
kubectl get clusterrole pvviewer-role

kubectl create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewer

kubectl run messaging --image=redis:alpine -o yaml > redis.yaml 
add in spec:
    serviceAccountName: pvviewer

kubectl get serviceaccount pvviewer -n default
kubectl get pods pvviewer -n default
kubectl auth can-i list persistentvolumes --as=system:serviceaccount:default:pvviewer

2. List the InternalIP of all nodes of the cluster. Save the result to a file /root/CKA/node_ips.
Answer should be in the format: InternalIP of controlplane<space>InternalIP of node01 (in a single line)

kubectl get nodes -o=jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' > /root/CKA/node_ips

| jq 

3. Create a pod called multi-pod with two containers.
Container 1: name: alpha, image: nginx
Container 2: name: beta, image: busybox, command: sleep 4800

Environment Variables:
container 1:
name: alpha

Container 2:
name: beta

---
apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  containers:
  - name: alpha
    image: nginx
    env:
    - name: name
      value: alpha
  - name: beta
    image: busybox
    command: ["sleep", "4800"]
    env:
    - name: name
      value: beta

4. Create a Pod called non-root-pod , image: redis:alpine

runAsUser: 1000
fsGroup: 2000

apiVersion: v1
kind: Pod
metadata:
  name: non-root-pod
spec:
  securityContext:
    runAsUser: 1000
    fsGroup: 2000
  containers:
    - name: redis
      image: redis:alpine

5. We have deployed a new pod called np-test-1 and a service called np-test-service. 
Incoming connections to this service are not working. Troubleshoot and fix it.
Create NetworkPolicy, by the name ingress-to-nptest that allows incoming connections 
to the service over port 80.

Important: Don't delete any current objects deployed.
Important: Don't Alter Existing Objects!
NetworkPolicy: Applied to All sources (Incoming traffic from all pods)?
NetWorkPolicy: Correct Port?
NetWorkPolicy: Applied to correct Pod?

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-to-nptest
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: np-test-1
  policyTypes:
  - Ingress
  ingress:
  - ports:
    - protocol: TCP
      port: 80


6. Taint the worker node node01 to be Unschedulable. Once done, create a pod called dev-redis, 
image redis:alpine, to ensure workloads are not scheduled to this worker node. Finally, create 
a new pod called prod-redis and image: redis:alpine with toleration to be scheduled on node01.
key: env_type, value: production, operator: Equal and effect: NoSchedule

To add taints on the node01 worker node:

kubectl taint node node01 env_type=production:NoSchedule

Now, deploy dev-redis pod and to ensure that workloads are not scheduled to this node01 worker node.

kubectl run dev-redis --image=redis:alpine

To view the node name of recently deployed pod:

kubectl get pods -o wide

Solution manifest file to deploy new pod called prod-redis with toleration to be scheduled on node01 worker node.

---
apiVersion: v1
kind: Pod
metadata:
  name: prod-redis
spec:
  containers:
  - name: prod-redis
    image: redis:alpine
  tolerations:
  - effect: NoSchedule
    key: env_type
    operator: Equal
    value: production     

To view only prod-redis pod with less details:

kubectl get pods -o wide | grep prod-redis

7. Create a pod called hr-pod in hr namespace belonging to the production environment and frontend tier.
image: redis:alpine

Use appropriate labels and create all the required objects if it does not exist in the system already.
hr-pod labeled with environment production?
hr-pod labeled with tier frontend?


kubectl run hr-pod --image=redis:alpine --namespace=hr --labels=environment=production,tier=frontend

8. A kubeconfig file called super.kubeconfig has been created under /root/CKA. 
There is something wrong with the configuration. Troubleshoot and fix it.
Fix /root/CKA/super.kubeconfig

9. We have created a new deployment called nginx-deploy. Scale the deployment to 3 replicas. 
Has the number of replica's increased? Troubleshoot the issue and fix it.
Does the deployment have 3 replicas?

10. Configure Horizontal Pod Autoscaler (HPA)
Instructions:
Create a Horizontal Pod Autoscaler (HPA) for the deployment named api-deployment located in the api namespace.
The HPA should scale the deployment based on a custom metric named requests_per_second, 
targeting an average value of 1000 requests per second across all pods.
Set the minimum number of replicas to 1 and the maximum to 20.
Note: Deployment named api-deployment is available in api namespace.

Is api-hpa HPA deployed in api namespace?
Is api-hpa configured for metric requests_per_second?

11. Deploy a Vertical Pod Autoscaler (VPA)
Create a Vertical Pod Autoscaler cache-vpa for a stateful application named cache-statefulset 
in the default namespace. The VPA should set optimal CPU and memory requests for newly created 
pods while ensuring that existing pods are not evicted. Please configure the VPA to operate 
in the Initial mode to achieve this behavior.
Note: Stateful set cache-statefulset is already deployed on the terminal.

Is the Vertical Pod Autoscalercache-vpa created?
Is the VPA cache-vpa created for statefulset cache-statefulset?
Is the policy mode set to Initial ?

12. Configure the web-route to split traffic between web-service and web-service-v2.
The configuration should ensure that 80% of the traffic is routed to web-service and 20% 
is routed to web-service-v2.
Note: web-gateway, web-service, and web-service-v2 have already been created and are available 
in the terminal.

Is the web-route deployed to as HTTP route?
Is the route configured to gateway web-gateway?
Is the route configured to service web-service?

13. One application, webpage-server-01, is deployed on the Kubernetes cluster by the Helm tool. 
Now, the team wants to deploy a new version of the application by replacing the existing one. 
A new version of the helm chart is given in the /root/new-version directory on the terminal. 
Validate the chart before installing it on the Kubernetes cluster. 
Use the helm command to validate and install the chart. After successfully installing the newer version, 
uninstall the older version. 

Is the new version app deployed?
Is the old version app uninstalled?
