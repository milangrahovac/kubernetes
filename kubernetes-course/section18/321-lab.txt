tacno 1,2, 4,

1. Create a Pod called redis-storage with image: redis:alpine with 
a Volume of type emptyDir that lasts for the life of the Pod.

apiVersion: v1
kind: Pod
metadata:
  name: redis-storage
spec:
  containers:
  - name: redis
    image: redis:alpine
    volumeMounts:
    - name: redis-data
      mountPath: /data/redis
  volumes:
  - name: redis-data
    emptyDir: {}

kubectl apply -f redis.yaml 

2. Create a new pod called super-user-pod with image busybox:1.28. 
Allow the pod to be able to set system_time.

apiVersion: v1
kind: Pod
metadata:
  name: super-user-pod
spec:
  containers:
  - name: busybox
    image: busybox:1.28
    command: ["sleep", "3600"]
    securityContext:
     capabilities:
        add: ["SYS_TIME"]

k apply -f super-pod.yaml 

3. A pod definition file is created at /root/CKA/use-pv.yaml. 
Make use of this manifest file and mount the persistent volume called pv-1. 
Ensure the pod is running and the PV is bound.

mountPath: /data
persistentVolumeClaim Name: my-pvc

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
       storage: 10Mi


apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: use-pv
  name: use-pv
spec:
  containers:
  - image: nginx
    name: use-pv
    volumeMounts:
    - mountPath: "/data"
      name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: my-pvc

4. Create a new deployment called nginx-deploy, with image nginx:1.16 and 1 replica. 
Next upgrade the deployment to version 1.17 using rolling update.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.16
        ports:
        - containerPort: 80
  strategy:
    type: RollingUpdate

k apply -f task4/nginx.yaml 

5. Create a new user called john. Grant him access to the cluster. 
John should have permission to create, list, get, update and delete pods in the development namespace . 
The private key exists in the location: /root/CKA/john.key and csr at /root/CKA/john.csr.
Important Note: As of kubernetes 1.19, the CertificateSigningRequest object expects a signerName.
Please refer the documentation to see an example. The documentation tab is available 
at the top right of terminal.

Solution
Solution manifest file to create a CSR as follows:

---
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: john-developer
spec:
  signerName: kubernetes.io/kube-apiserver-client
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZEQ0NBVHdDQVFBd0R6RU5NQXNHQTFVRUF3d0VhbTlvYmpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRApnZ0VQQURDQ0FRb0NnZ0VCQUt2Um1tQ0h2ZjBrTHNldlF3aWVKSzcrVVdRck04ZGtkdzkyYUJTdG1uUVNhMGFPCjV3c3cwbVZyNkNjcEJFRmVreHk5NUVydkgyTHhqQTNiSHVsTVVub2ZkUU9rbjYra1NNY2o3TzdWYlBld2k2OEIKa3JoM2prRFNuZGFvV1NPWXBKOFg1WUZ5c2ZvNUpxby82YU92czFGcEc3bm5SMG1JYWpySTlNVVFEdTVncGw4bgpjakY0TG4vQ3NEb3o3QXNadEgwcVpwc0dXYVpURTBKOWNrQmswZWhiV2tMeDJUK3pEYzlmaDVIMjZsSE4zbHM4CktiSlRuSnY3WDFsNndCeTN5WUFUSXRNclpUR28wZ2c1QS9uREZ4SXdHcXNlMTdLZDRaa1k3RDJIZ3R4UytkMEMKMTNBeHNVdzQyWVZ6ZzhkYXJzVGRMZzcxQ2NaanRxdS9YSmlyQmxVQ0F3RUFBYUFBTUEwR0NTcUdTSWIzRFFFQgpDd1VBQTRJQkFRQ1VKTnNMelBKczB2czlGTTVpUzJ0akMyaVYvdXptcmwxTGNUTStsbXpSODNsS09uL0NoMTZlClNLNHplRlFtbGF0c0hCOGZBU2ZhQnRaOUJ2UnVlMUZnbHk1b2VuTk5LaW9FMnc3TUx1a0oyODBWRWFxUjN2SSsKNzRiNnduNkhYclJsYVhaM25VMTFQVTlsT3RBSGxQeDNYVWpCVk5QaGhlUlBmR3p3TTRselZuQW5mNm96bEtxSgpvT3RORStlZ2FYWDdvc3BvZmdWZWVqc25Yd0RjZ05pSFFTbDgzSkljUCtjOVBHMDJtNyt0NmpJU3VoRllTVjZtCmlqblNucHBKZWhFUGxPMkFNcmJzU0VpaFB1N294Wm9iZDFtdWF4bWtVa0NoSzZLeGV0RjVEdWhRMi80NEMvSDIKOWk1bnpMMlRST3RndGRJZjAveUF5N05COHlOY3FPR0QKLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==
  usages:
  - digital signature
  - key encipherment
  - client auth

To approve this certificate, run: kubectl certificate approve john-developer

Next, create a role developer and rolebinding developer-role-binding, run the command:

$ kubectl create role developer --resource=pods --verb=create,list,get,update,delete --namespace=development

$ kubectl create rolebinding developer-role-binding --role=developer --user=john --namespace=development

To verify the permission from kubectl utility tool:

$ kubectl auth can-i update pods --as=john --namespace=development

6. Create a nginx pod called nginx-resolver using image nginx, expose it internally 
with a service called nginx-resolver-service.
Test that you are able to look up the service and pod names from within the cluster. 
Use the image: busybox:1.28 for dns lookup. Record results in /root/CKA/nginx.svc and /root/CKA/nginx.pod

method1:
k run nginx-resolver --image=nginx
k expose pod nginx-resolver --name=nginx-resolver-service --port=80
k run busybox --image=busybox:1.28 -- sleep 4000
k exec busybox -- nslookup nginx-resolver-service
k exec busybox -- nslookup nginx-resolver-service > /root/CKA/nginx.svc
k exec busybox -- nslookup 172-17-1-14.default.pod.cluster.local > /root/CKA/nginx.pod


7. Create a static pod on node01 called nginx-critical with image nginx and make sure 
that it is recreated/restarted automatically in case of a failure.

Use /etc/kubernetes/manifests as the Static Pod path for example.
static pod configured under /etc/kubernetes/manifests ?
Pod nginx-critical-node01 is up and running


kubectl get nodes -o wide
ssh 192.168.129.220
cd /etc/kubernetes/manifests/


nano /etc/kubernetes/manifests/nginx-critical.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-critical
  namespace: default
spec:
  containers:
    - name: nginx
      image: nginx
      ports:
        - containerPort: 80
        
  restartPolicy: Always

kubectl apply -f /etc/kubernetes/manifests/nginx-critical.yaml

8. Configure Horizontal Pod Autoscaler (HPA) with Custom Scale-Down Behavior
Instructions:
Create a Horizontal Pod Autoscaler (HPA) with name backend-hpa for the deployment 
named backend-deployment in the backend namespace with the webapp-hpa.yaml file located under the root folder.
Ensure that the HPA scales the deployment based on memory utilization, maintaining an average 
memory usage of 65% across all pods.
Configure the HPA with a minimum of 3 replicas and a maximum of 15.

Is backend-hpa HPA deployed in backend namespace?
Is deployment configured for metrics memory utilization?

9. Deploy a Vertical Pod Autoscaler (VPA).
Deploy a Vertical Pod Autoscaler (VPA) for the deployment named multi-container-deployment 
in the default namespace.
This deployment consists of two containers: frontend and backend. Configure the VPA to manage 
resource requests for the backend-app container only, excluding the frontend-app container 
from automatic resource adjustments.

Is the VPA multi-container-vpa created for deployment multi-container-deployment?

Is the resourcePolicy set to off mode for frontend container in multi-container-deployment?

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: backend-hpa
  namespace: backend
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: backend-deployment
  minReplicas: 3
  maxReplicas: 15
  metrics:
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 65


10. Create an HTTPRoute resource named web-route in the default namespace. 
This route should direct traffic from the web-gateway to a backend service named web-service on port 80.
web-gateway is in the nginx-gateway namespace. And service web-service and deployment web-app 
are in the default namespace.


Is the web-route deployed to as HTTP route?
Is the route configured to gateway web-gateway?
Is the route configured to service web-service?

kubectl create -n default -f - <<EOF
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: web-route
  namespace: default
spec:
  parentRefs:
    - name: web-gateway
      namespace: nginx-gateway
  rules:
    - backendRefs:
        - name: web-service
          port: 80
EOF

11. On the cluster, the team has installed multiple helm charts on a different namespace. 
By mistake, those deployed resources include one of the vulnerable images called kodekloud/webapp-color:v1. 
Find out the release name and uninstall it.
